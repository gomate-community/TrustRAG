#!/usr/bin/env python
# -*- coding:utf-8 _*-
"""
æœ€ç®€åŒ–çš„RAGé—®ç­”ç³»ç»Ÿ
ç›´æ¥æ‰§è¡Œï¼Œæ— å‡½æ•°å°è£…
"""
import os
import sys
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
sys.path.append(".")
from tqdm import tqdm

from trustrag.modules.document.common_parser import CommonParser
from trustrag.modules.document.chunk import TextChunker
from trustrag.modules.vector.embedding import SentenceTransformerEmbedding
from trustrag.modules.reranker.bge_reranker import BgeRerankerConfig, BgeReranker
from trustrag.modules.retrieval.dense_retriever import DenseRetrieverConfig, DenseRetriever
from trustrag.modules.generator.llm import Qwen3Chat

# é…ç½®å‚æ•°
DOCS_PATH = r"data/docs"
LLM_MODEL_PATH = r"G:\pretrained_models\llm\Qwen3-4B"
EMBEDDING_MODEL_PATH = r"G:\pretrained_models\mteb\bge-large-zh-v1.5"
RERANKER_MODEL_PATH = r"G:\pretrained_models\mteb\bge-reranker-large"
INDEX_PATH = r"examples/retrievers/dense_cache"
EMBEDDING_DIM = 1024
CHUNK_SIZE = 256
TOP_K = 5

print("ğŸš€ å¯åŠ¨RAGé—®ç­”ç³»ç»Ÿ")
print("="*50)

# Step 1: åˆå§‹åŒ–ç»„ä»¶
print("Step 1: æ­£åœ¨åˆå§‹åŒ–ç»„ä»¶...")

# åˆå§‹åŒ–æ–‡æ¡£è§£æå™¨
parser = CommonParser()
print("  âœ“ æ–‡æ¡£è§£æå™¨åˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
tc = TextChunker()
print("  âœ“ æ–‡æœ¬åˆ†å—å™¨åˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
embedding_generator = SentenceTransformerEmbedding(EMBEDDING_MODEL_PATH)
print("  âœ“ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–æ£€ç´¢å™¨
retriever_config = DenseRetrieverConfig(
    model_name_or_path=EMBEDDING_MODEL_PATH,
    dim=EMBEDDING_DIM,
    index_path=INDEX_PATH
)
retriever = DenseRetriever(retriever_config, embedding_generator)
print("  âœ“ æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–é‡æ’åºå™¨
rerank_config = BgeRerankerConfig(
    model_name_or_path=RERANKER_MODEL_PATH
)
reranker = BgeReranker(rerank_config)
print("  âœ“ é‡æ’åºå™¨åˆå§‹åŒ–å®Œæˆ")

# åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
llm = Qwen3Chat(LLM_MODEL_PATH)
print("  âœ“ å¤§è¯­è¨€æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

print("Step 1: æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ!\n")

# Step 2: å¤„ç†å‘é‡ç´¢å¼•
print("Step 2: å¤„ç†å‘é‡ç´¢å¼•...")

# æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰ç´¢å¼•
if os.path.exists(INDEX_PATH):
    print("  å‘ç°ç°æœ‰ç´¢å¼•ï¼Œæ­£åœ¨åŠ è½½...")
    retriever.load_index(INDEX_PATH)
    print("  âœ“ ç´¢å¼•åŠ è½½å®Œæˆ\n")
else:
    print("  æœªå‘ç°ç°æœ‰ç´¢å¼•ï¼Œå¼€å§‹æ„å»ºæ–°ç´¢å¼•...")
    
    # Step 3: æ„å»ºå‘é‡å­˜å‚¨
    print("Step 3: æ„å»ºå‘é‡å­˜å‚¨...")
    
    # æ£€æŸ¥æ–‡æ¡£ç›®å½•
    if not os.path.exists(DOCS_PATH):
        print(f"  âŒ æ–‡æ¡£ç›®å½• {DOCS_PATH} ä¸å­˜åœ¨")
        print(f"  è¯·åˆ›å»ºç›®å½•å¹¶æ·»åŠ æ–‡æ¡£æ–‡ä»¶")
        exit(1)
    
    # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
    doc_files = [f for f in os.listdir(DOCS_PATH) if os.path.isfile(os.path.join(DOCS_PATH, f))]
    if not doc_files:
        print("  âŒ æ–‡æ¡£ç›®å½•ä¸ºç©ºï¼Œè¯·æ·»åŠ æ–‡æ¡£æ–‡ä»¶")
        exit(1)
    
    print(f"  å‘ç° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    # è§£ææ‰€æœ‰æ–‡æ¡£
    all_paragraphs = []
    for filename in doc_files:
        file_path = os.path.join(DOCS_PATH, filename)
        try:
            paragraphs = parser.parse(file_path)
            all_paragraphs.append(paragraphs)
            print(f"  âœ“ å·²è§£æ: {filename}")
        except Exception as e:
            print(f"  âŒ è§£æå¤±è´¥ {filename}: {e}")
    
    if not all_paragraphs:
        print("  âŒ æ²¡æœ‰æˆåŠŸè§£æçš„æ–‡æ¡£")
        exit(1)
    
    # æ–‡æ¡£åˆ†å—
    print("  æ­£åœ¨è¿›è¡Œæ–‡æ¡£åˆ†å—...")
    all_chunks = []
    for paragraphs in tqdm(all_paragraphs, desc="  åˆ†å—å¤„ç†"):
        if isinstance(paragraphs, list) and paragraphs:
            if isinstance(paragraphs[0], dict):
                text_list = [' '.join(str(value) for value in item.values()) for item in paragraphs]
            else:
                text_list = [str(item) for item in paragraphs]
        else:
            text_list = [str(paragraphs)] if paragraphs else []
        
        chunks = tc.get_chunks(text_list, CHUNK_SIZE)
        all_chunks.extend(chunks)
    
    print(f"  âœ“ ç”Ÿæˆäº† {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    # æ„å»ºå‘é‡ç´¢å¼•
    print("  æ­£åœ¨æ„å»ºå‘é‡ç´¢å¼•...")
    retriever.build_from_texts(all_chunks)
    
    # ä¿å­˜ç´¢å¼•
    index_dir = os.path.dirname(INDEX_PATH)
    if not os.path.exists(index_dir):
        os.makedirs(index_dir)
    retriever.save_index(INDEX_PATH)
    print(f"  âœ“ ç´¢å¼•å·²ä¿å­˜åˆ°: {INDEX_PATH}")
    print("Step 3: å‘é‡å­˜å‚¨æ„å»ºå®Œæˆ!\n")

# Step 4: å¼€å§‹é—®ç­”å¾ªç¯
print("Step 4: å¯åŠ¨é—®ç­”ç³»ç»Ÿ")
print("="*50)
print("RAGé—®ç­”ç³»ç»Ÿå·²å¯åŠ¨!")
print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºç¨‹åº")
print("è¾“å…¥ 'rebuild' é‡æ–°æ„å»ºç´¢å¼•")
print("="*50)

while True:
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        question = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
        
        # æ£€æŸ¥é€€å‡ºå‘½ä»¤
        if question.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("å†è§!")
            break
        
        # æ£€æŸ¥é‡å»ºç´¢å¼•å‘½ä»¤
        if question.lower() in ['rebuild', 'é‡å»º']:
            print("\næ­£åœ¨é‡æ–°æ„å»ºç´¢å¼•...")
            
            # é‡æ–°æ„å»ºç´¢å¼•çš„ä»£ç 
            doc_files = [f for f in os.listdir(DOCS_PATH) if os.path.isfile(os.path.join(DOCS_PATH, f))]
            all_paragraphs = []
            for filename in doc_files:
                file_path = os.path.join(DOCS_PATH, filename)
                try:
                    paragraphs = parser.parse(file_path)
                    all_paragraphs.append(paragraphs)
                except:
                    pass
            
            all_chunks = []
            for paragraphs in tqdm(all_paragraphs, desc="é‡æ–°åˆ†å—"):
                if isinstance(paragraphs, list) and paragraphs:
                    if isinstance(paragraphs[0], dict):
                        text_list = [' '.join(str(value) for value in item.values()) for item in paragraphs]
                    else:
                        text_list = [str(item) for item in paragraphs]
                else:
                    text_list = [str(paragraphs)] if paragraphs else []
                
                chunks = tc.get_chunks(text_list, CHUNK_SIZE)
                all_chunks.extend(chunks)
            
            retriever.build_from_texts(all_chunks)
            retriever.save_index(INDEX_PATH)
            print("ç´¢å¼•é‡å»ºå®Œæˆ!")
            continue
        
        # æ£€æŸ¥ç©ºè¾“å…¥
        if not question:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
            continue
        
        print("\næ­£åœ¨æ€è€ƒä¸­...")
        
        # RAGé—®ç­”å¤„ç†
        print(f"æ­£åœ¨å¤„ç†é—®é¢˜: {question}")
        
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        print("  æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...")
        contents = retriever.retrieve(query=question, top_k=TOP_K)
        print(f"  âœ“ æ£€ç´¢åˆ° {len(contents)} ä¸ªç›¸å…³æ–‡æ¡£å—")
        
        # é‡æ’åº
        print("  æ­£åœ¨é‡æ’åºæ–‡æ¡£...")
        contents = reranker.rerank(query=question, documents=[content['text'] for content in contents])
        print("  âœ“ æ–‡æ¡£é‡æ’åºå®Œæˆ")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        print("  æ­£åœ¨æ„å»ºä¸Šä¸‹æ–‡...")
        context = '\n'.join([content['text'] for content in contents])
        print("  âœ“ ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ")
        
        # ç”Ÿæˆå›ç­”
        print("  æ­£åœ¨ç”Ÿæˆå›ç­”...")
        result, history = llm.chat(question, [], context)
        print("  âœ“ å›ç­”ç”Ÿæˆå®Œæˆ")
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*50)
        print("å›ç­”:")
        print(result)
        print("\n" + "-"*30)
        print(f"å‚è€ƒäº† {len(contents)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        
        # å¯é€‰æ˜¾ç¤ºå‚è€ƒæ–‡æ¡£
        show_sources = input("\næ˜¯å¦æ˜¾ç¤ºå‚è€ƒæ–‡æ¡£ç‰‡æ®µ? (y/n): ").strip().lower()
        if show_sources in ['y', 'yes', 'æ˜¯']:
            print("\nå‚è€ƒæ–‡æ¡£ç‰‡æ®µ:")
            for idx, source in enumerate(contents[:3], 1):
                score = source.get('score', 0)
                text = source['text']
                preview = text[:200] + "..." if len(text) > 200 else text
                print(f"\n[ç‰‡æ®µ {idx}] (ç›¸å…³åº¦: {score:.3f})")
                print(preview)
    
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        break
    except Exception as e:
        print(f"\nå‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·é‡è¯•æˆ–è¾“å…¥ 'quit' é€€å‡º")

print("\nç¨‹åºç»“æŸ")